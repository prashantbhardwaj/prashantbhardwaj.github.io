<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Prashant Bhardwaj</title>
        <link>/posts/</link>
        <description>Recent content in Posts on Prashant Bhardwaj</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <copyright>copyright@Prashant</copyright>
        <lastBuildDate>Sun, 29 Sep 2019 13:49:13 +0100</lastBuildDate>
        <atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>Cassandra Replication Strategies</title>
            <link>/posts/2019/09/cassandra-replication-strategies/</link>
            <pubDate>Sun, 29 Sep 2019 13:49:13 +0100</pubDate>
            
            <guid>/posts/2019/09/cassandra-replication-strategies/</guid>
            <description>There are two kinds of replication strategies in Cassandra.
SimpleStrategy SimpleStrategy is used when you have just one data center. SimpleStrategy places the first replica on the node selected by the partitioner. After that, remaining replicas are placed in clockwise direction in the Node ring.
NetworkTopologyStrategy NetworkTopologyStrategy is used when you have more than two data centers.
In NetworkTopologyStrategy, replicas are set for each data center separately. NetworkTopologyStrategy places replicas in the clockwise direction in the ring until reaches the first node in another rack.</description>
            <content type="html"><![CDATA[<p>There are two kinds of replication strategies in Cassandra.</p>

<h3>SimpleStrategy</h3>

<p>SimpleStrategy is used when you have just one data center. SimpleStrategy places the first replica on the node selected by the partitioner. After that, remaining replicas are placed in clockwise direction in the Node ring.</p>

<p><img src="/caas/simple-replication-strategy.jpg" alt="alt text" /></p>

<h3>NetworkTopologyStrategy</h3>

<p>NetworkTopologyStrategy is used when you have more than two data centers.</p>

<p>In NetworkTopologyStrategy, replicas are set for each data center separately. NetworkTopologyStrategy places replicas in the clockwise direction in the ring until reaches the first node in another rack.</p>

<p>This strategy tries to place replicas on different racks in the same data center. This is due to the reason that sometimes failure or problem can occur in the rack. Then replicas on other nodes can provide data.</p>

<p>Here is the pictorial representation of the Network topology strategy</p>

<p><img src="/caas/network-replication-strategy.jpg" alt="alt text" /></p>
]]></content>
        </item>
        
        <item>
            <title>Cassandra Consistency Levels List</title>
            <link>/posts/2019/09/cassandra-consistency-levels-list/</link>
            <pubDate>Sat, 28 Sep 2019 11:19:22 +0100</pubDate>
            
            <guid>/posts/2019/09/cassandra-consistency-levels-list/</guid>
            <description>Typical values of consistency levels are -
Note - Write operation consists of writing to commit log and memtable.
   Level Write Read Usage     ALL A write must be on all replica nodes in the cluster for that partition. Returns the record after all replicas. The read operation will fail if a replica does not respond. Provides the highest consistency and the lowest availability of any other level.</description>
            <content type="html"><![CDATA[<p>Typical values of consistency levels are -</p>

<p>Note - Write operation consists of writing to commit log and memtable.</p>

<table>
<thead>
<tr>
<th>Level</th>
<th align="center">Write</th>
<th align="right">Read</th>
<th>Usage</th>
</tr>
</thead>

<tbody>
<tr>
<td>ALL</td>
<td align="center">A write must be on all replica nodes in the cluster for that partition.</td>
<td align="right">Returns the record after all replicas. The read operation will fail if a replica does not respond.</td>
<td>Provides the highest consistency and the lowest availability of any other level.</td>
</tr>

<tr>
<td>EACH_QUORUM</td>
<td align="center">A write must be on a quorum of replica nodes in each datacenter.</td>
<td align="right">Not supported for reads.</td>
<td>Strong consistency. Used in multiple datacenter clusters to strictly maintain consistency at the same level in each datacenter. For example, choose this level if you want a write to fail when a datacenter is down and the QUORUM cannot be reached on that datacenter.</td>
</tr>

<tr>
<td>QUORUM</td>
<td align="center">A write must be on a quorum of replica nodes across all datacenters.</td>
<td align="right">Returns the record after a quorum of replicas from all datacenters has responded.</td>
<td>Used in either single or multiple datacenter clusters to maintain strong consistency across the cluster. Use if you can tolerate some level of failure.</td>
</tr>

<tr>
<td>LOCAL_QUORUM</td>
<td align="center">A write must be on a quorum of replica nodes in the same datacenter as the coordinator.</td>
<td align="right">Returns the record after a quorum of replicas in the current datacenter as the coordinator has reported.</td>
<td>Strong consistency. Avoids latency of inter-datacenter communication. Used in multiple datacenter clusters with a rack-aware replica placement strategy, such as NetworkTopologyStrategy, and a properly configured snitch. Use to maintain consistency locally (within the single datacenter). Can be used with SimpleStrategy.</td>
</tr>

<tr>
<td>ONE</td>
<td align="center">A write must be at least one replica node.</td>
<td align="right">Returns a response from the closest replica, as determined by the snitch. By default, a read repair runs in the background to make the other replicas consistent.</td>
<td>Satisfies the needs of most users because consistency requirements are not stringent.</td>
</tr>

<tr>
<td>TWO</td>
<td align="center">A write must be at least two replica nodes.</td>
<td align="right">Returns the most recent data from two of the closest replicas.</td>
<td>Similar to ONE.</td>
</tr>

<tr>
<td>THREE</td>
<td align="center">A write must be at least three replica nodes.</td>
<td align="right">Returns the most recent data from three of the closest replicas.</td>
<td>Similar to TWO.</td>
</tr>

<tr>
<td>LOCAL_ONE</td>
<td align="center">A write must be sent to, and successfully acknowledged by, at least one replica node in the local datacenter.</td>
<td align="right">Returns a response from the closest replica in the local datacenter.</td>
<td>In a multiple datacenter clusters, a consistency level of ONE is often desirable, but cross-DC traffic is not. LOCAL_ONE accomplishes this. For security and quality reasons, you can use this consistency level in an offline datacenter to prevent automatic connection to online nodes in other datacenters if an offline node goes down.</td>
</tr>

<tr>
<td>SERIAL</td>
<td align="center">Not supported</td>
<td align="right">Allows reading the current (and possibly uncommitted) state of data without proposing a new addition or update. If a SERIAL read finds an uncommitted transaction in progress, it will commit the transaction as part of the read. Similar to QUORUM.</td>
<td>To read the latest value of a column after a user has invoked a lightweight transaction to write to the column, use SERIAL. Cassandra then checks the inflight lightweight transaction for updates and, if found, returns the latest data.</td>
</tr>

<tr>
<td>LOCAL_SERIAL</td>
<td align="center">Not supported</td>
<td align="right">Same as SERIAL, but confined to the datacenter. Similar to LOCAL_QUORUM.</td>
<td>Used to achieve linearizable consistency for lightweight transactions.</td>
</tr>

<tr>
<td>ANY</td>
<td align="center">A write must be written to at least one node. If all replica nodes for the given partition key are down, the write can still succeed after a hinted handoff has been written. If all replica nodes are down at write time, an ANY write is not readable until the replica nodes for that partition have recovered.</td>
<td align="right">Not supported</td>
<td>Provides low latency and a guarantee that a write never fails. Delivers the lowest consistency and highest availability.</td>
</tr>
</tbody>
</table>
]]></content>
        </item>
        
        <item>
            <title>Cassandra - Replication Factor &amp; Consistency Level</title>
            <link>/posts/2019/09/cassandra-replication-factor-consistency-level/</link>
            <pubDate>Sat, 28 Sep 2019 09:31:52 +0100</pubDate>
            
            <guid>/posts/2019/09/cassandra-replication-factor-consistency-level/</guid>
            <description>Replication Factor Whenever we start Cassandra, we start a cluster, which is a collection of 1 or more than one nodes. A cluster can be splitted onto more than one datacenter. Cassandra’s architecture was that the hardware failure can occur at any time. Any node can be down. Even entire datacenter can go down so it is better to add more than one data centers in your cluster. Cassandra stores data replicas on multiple nodes to ensure reliability and fault tolerance.</description>
            <content type="html"><![CDATA[<p><h2>Replication Factor</h2>
Whenever we start Cassandra, we start a cluster, which is a collection of 1 or more than one nodes. A cluster can be splitted onto more than one datacenter. Cassandra’s architecture was that the hardware failure can occur at any time. Any node can be down. Even entire datacenter can go down so it is better to add more than one data centers in your cluster. Cassandra stores data replicas on multiple nodes to ensure reliability and fault tolerance. The total number of replicas for a keyspace across a Cassandra cluster is referred to as the keyspace&rsquo;s replication factor. A replication factor of one means that there is only one copy of each row in the Cassandra cluster. A replication factor of two means there are two copies of each row, where each copy is on a different node. All replicas are equally important; there is no primary or master replica.</p>

<p>In a production system, it is advisable to have three or more Cassandra nodes in each data center, and the default replication factor should be three. As a general rule, the replication factor should not exceed the number of Cassandra nodes in the cluster. If there are 3 nodes in your cluster then how can you store data on more than three nodes?</p>

<p>If you add additional Cassandra nodes to the cluster, the default replication factor is not affected.</p>

<p>For example, if you increase the number of Cassandra nodes to six, but leave the replication factor at three, that means data will be replicated on just three nodes not on all six nodes. If you are adding of deleting nodes in a cluster, it is your responsibility to change the replication factor. If a node goes down, a higher replication factor means a higher probability that the data on the node exists on one of the remaining nodes. The downside of a higher replication factor is an increased latency on data writes.</p>

<p>All the nodes exchange information with each other using Gossip protocol. Gossip is a protocol in Cassandra by which nodes can communicate with each other.</p>

<p>There are two kinds of replication strategies in Cassandra. Which are mentioned <a href="../cassandra-replication-strategy/">here</a>.</p>

<p><h2>Consistency Level</h2>
When you write any data in a cluster which has four nodes and replication factor three, data is stored on three nodes. This replication of data on three nodes is quick but not instant. When you write a data and instantly read the same data, it is possible that data is not yet completly replicated and some of the nodes are returning old data. Then how Cassandra enforces consistency? Simple answer is - Using consistency level.</p>

<p>The Cassandra consistency level is defined as the minimum number of Cassandra nodes that must acknowledge a read or write operation before the operation can be considered successful. Different consistency levels can be assigned to different Edge keyspaces.</p>

<p><a href="../cassandra-consistency-levels-list/">This table describes various types of write/read consistency levels. </a></p>

<p>When connecting to Cassandra for read and write operations, generally used consistency level value is LOCAL_QUORUM. However, some keyspaces are defined to use a consistency level of one.</p>

<p>The calculation of the value of LOCAL_QUORUM for a data center is:</p>

<pre><code>LOCAL_QUORUM = (replication_factor/2) + 1 
</code></pre>

<p>As described above, the default replication factor for an Edge production environment with three Cassandra nodes is three. Therefore, the default value of LOCAL_QUORUM = (<sup>3</sup>&frasl;<sub>2</sub>) +1 = 2 (the value is rounded down to an integer).</p>

<p>With LOCAL_QUORUM = 2, at least two of the three Cassandra nodes in the data center must respond to a read/write operation for the operation to succeed. For a three node Cassandra cluster, the cluster could therefore tolerate one node being down per data center.</p>

<p>By specifying the consistency level as LOCAL_QUORUM, Edge avoids the latency required by validating operations across multiple data centers. If a keyspace used the Cassandra QUORUM value as the consistency level, read/write operations would have to be validated across all data centers.</p>

<p>If you add additional Cassandra nodes to the cluster, the consistency level is not affected.
Consistency level can be different for read and write queries however it is advised to use same consistency level for read and write.</p>
]]></content>
        </item>
        
        <item>
            <title>Cassandra : When to use and how to use</title>
            <link>/posts/2019/09/cassandra-when-to-use-and-how-to-use/</link>
            <pubDate>Thu, 19 Sep 2019 23:36:07 +0100</pubDate>
            
            <guid>/posts/2019/09/cassandra-when-to-use-and-how-to-use/</guid>
            <description>I don&amp;rsquo;t think that I need to explain what Cassandra is. OK, one liner is enough ! It is NoSQL database, primarily used in big data projects.
Before we start discussing scenarios where Cassandra fits very well, let&amp;rsquo;s discuss where it doesn&amp;rsquo;t fit at all.
Common mistakes when using Cassandra We&amp;rsquo;ll have lots of data, let&amp;rsquo;s use Cassandra
Cassandra is not a typical database which fits in every Big Data use case.</description>
            <content type="html"><![CDATA[<p>I don&rsquo;t think that I need to explain what Cassandra is. OK, one liner is enough ! It is NoSQL database, primarily used in big data projects.</p>

<p>Before we start discussing scenarios where Cassandra fits very well, let&rsquo;s discuss where it doesn&rsquo;t fit at all.</p>

<h2>Common mistakes when using Cassandra</h2>

<p><strong>We&rsquo;ll have lots of data, let&rsquo;s use Cassandra</strong></p>

<p>Cassandra is not a typical database which fits in every Big Data use case. In some no big data cases also it can fit very well. At other side it can be a bad choice even in a big data project.
Cassandra is useless unless you need to read a lot of data in one go and that data is stored side by side so that a single seek read can grab entire data you need.
It is very bad if your query is returning just one or few rows and you are making lot of queries.</p>

<p><strong>Our organization has Cassandra running as a service so it is easy for us to atleast start with it</strong></p>

<p>So what? Let other teams use but you stay away if it doesn&rsquo;t suite you.
Sometime MySQL can be a better choice than Cassandra.</p>

<p><strong>Ohh ! CQL is just like SQL. Great !!, now we can query any data !</strong></p>

<p>No this is incorrect. CQL is not SQL. You can&rsquo;t join tables in CQL. There are many other differences too.</p>

<p><strong>Ohh no, Cassandra doesn&rsquo;t allow manipulation of data during read or write</strong></p>

<p>Yes, it just gives you what is stored or simply writes what you give it. No data manupulation on the fly.</p>

<p><strong>WoW, this User defined functions are great things, let&rsquo;s create many</strong></p>

<p>Don&rsquo;t keep Cassandra coordinator busy in doing things which are not related to Cassandra. This will impact the performance of your Cassandra cluster.</p>

<p><strong>NOTE</strong> - For any question/suggestion, please send email to    <strong>amu.prashant1@gmail.com</strong></p>
]]></content>
        </item>
        
        <item>
            <title>Cassandra : What is this?</title>
            <link>/posts/2019/09/cassandra-what-is-this/</link>
            <pubDate>Sat, 14 Sep 2019 17:01:38 +0100</pubDate>
            
            <guid>/posts/2019/09/cassandra-what-is-this/</guid>
            <description>Agenda  Characterstics How does it internally work References  The Apache Cassandra database is the right choice when you need scalability and high availability without compromising performance. Linear scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data. Cassandra&amp;rsquo;s support for replicating across multiple datacenters is best-in-class, providing lower latency for your users and the peace of mind of knowing that you can survive regional outages.</description>
            <content type="html"><![CDATA[

<h2>Agenda</h2>

<ul>
<li><a href="#h2-characterstics-h2">Characterstics</a></li>
<li><a href="#h2-how-does-it-internally-work-h2">How does it internally work</a></li>
<li><a href="#h2-references-h2">References</a></li>
</ul>

<p>The Apache Cassandra database is the right choice when you need scalability and high availability without compromising performance. Linear scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data. Cassandra&rsquo;s support for replicating across multiple datacenters is best-in-class, providing lower latency for your users and the peace of mind of knowing that you can survive regional outages.</p>

<h2 id="h2-characterstics-h2"><h2>Characterstics</h2></h2>

<p><strong>Fault tolerant</strong>
Data is automatically replicated to multiple nodes for fault-tolerance. Replication across multiple data centers is supported. Failed nodes can be replaced with no downtime.</p>

<p><strong>Performant</strong>
Cassandra consistently outperforms popular NoSQL alternatives in benchmarks and real applications, primarily because of fundamental architectural choices.</p>

<p><strong>Decentralized</strong>
There are no single points of failure. There are no network bottlenecks. Every node in the cluster is identical.</p>

<p><strong>Scalable</strong>
Some of the largest production deployments include Apple&rsquo;s, with over 75,000 nodes storing over 10 PB of data, Netflix (2,500 nodes, 420 TB, over 1 trillion requests per day), Chinese search engine Easou (270 nodes, 300 TB, over 800 million requests per day), and eBay (over 100 nodes, 250 TB).</p>

<p><strong>Durable</strong>
Cassandra is suitable for applications that can&rsquo;t afford to lose data, even when an entire data center goes down.</p>

<p><strong>You&rsquo;re in control</strong>
Choose between synchronous or asynchronous replication for each update. Highly available asynchronous operations are optimized with features like Hinted Handoff and Read Repair.</p>

<p><strong>Elastic</strong>
Read and write throughput both increase linearly as new machines are added, with no downtime or interruption to applications.</p>

<p><strong>Professionally Supported</strong>
Cassandra support contracts and services are available from third parties.</p>

<h2 id="h2-how-does-it-internally-work-h2"><h2>How does it internally work</h2></h2>

<h3>Writes on a single node</h3>

<p><img src="/caas/cass-write.png" alt="alt text" /></p>

<p>Entire write happens in following steps</p>

<ul>
<li>Logging data in the commit log</li>
<li>Writing data to the memtable</li>
<li>Flushing data from the memtable</li>
<li>Storing data on disk in SSTables</li>
<li>Compaction</li>
</ul>

<p>First thing it does, is writing the data in commit log, which is on disc, because of that it is durable.
It is append only log which is sequencial write and that is the reason that write in Cassandra is super fast.</p>

<p>Then it writes data in memtable. The memtable is a write-back cache of data partitions that Cassandra looks up by key. The more a table is used, the larger its memtable needs to be. Cassandra can dynamically allocate the right amount of memory for the memtable or you can manage the amount of memory being utilized yourself.</p>

<p>Row in memtable can have 2 billion columns. After writing data in the column it sends acknowldgement back to client.</p>

<p>The memtable, unlike a write-through cache, stores writes until reaching a limit, and then is flushed. When memtable contents exceed a configurable threshold, the memtable data, which includes indexes, is put in a queue to be flushed to disk. To flush the data, Cassandra sorts memtables by partition key and then writes the data to disk sequentially. The process is extremely fast because it involves only a commitlog append and the sequential write.</p>

<p>SSTables are immutable, not written to again after the memtable is flushed. Consequently, a partition is typically stored across multiple SSTable files So, if a row is not in memtable, a read of the row needs look-up in all the SSTable files. This is why read in Cassandra is much slower than write.</p>

<p>Data in the commit log is purged after its corresponding data in the memtable is flushed to the SSTable. The commit log is for recovering the data in memtable in the event of a hardware failure.</p>

<p>as we know that data for a row in SSTable is not in just one SSTable. Whenever data in a row is updated Cassandra writes a new timestamped version of the inserted or updated data in another SSTable. Cassandra also does not delete in place because the SSTable is immutable. Instead, Cassandra marks data to be deleted using a tombstone. Tombstones exist for a configured time period defined by the gc_grace_seconds value set on the table. During compaction, there is a temporary spike in disk space usage and disk I/O because the old and new SSTables co-exist. This diagram depicts the compaction process:</p>

<p><img src="/caas/caas-compaction.png" alt="alt text" /></p>

<p>Compaction merges the data in each SSTable data by partition key, selecting the latest data for storage based on its timestamp. Cassandra can merge the data performantly, without random IO, because rows are sorted by partition key within each SSTable. After evicting tombstones and removing deleted data, columns, and rows, the compaction process consolidates SSTables into a single file. The old SSTable files are deleted as soon as any pending reads finish using the files. Disk space occupied by old SSTables becomes available for reuse.</p>

<p>Data input to SSTables is sorted to prevent random I/O during SSTable consolidation. After compaction, Cassandra uses the new consolidated SSTable instead of multiple old SSTables, fulfilling read requests more efficiently than before compaction. The old SSTable files are deleted as soon as any pending reads finish using the files. Disk space occupied by old SSTables becomes available for reuse.</p>

<p>Although no random I/O occurs, compaction can still be a fairly heavyweight operation. During compaction, there is a temporary spike in disk space usage and disk I/O because the old and new SSTables co-exist. To minimize deteriorating read speed, compaction runs in the background.</p>

<h3>Coordinated reads</h3>

<p><img src="/caas/caas-read.png" alt="alt text" /></p>

<p>Application client can send data request to any node of your cluster. That node then becomes coordinator for that request. Now it is coordinator&rsquo;s job to talk to other nodes, get the data and return it to client.
When a read request starts its journey, the data’s partition key is used to find what nodes have the data. After that, the request is sent to a number of nodes set by the tunable consistency level for reads. Then, on each node, in a certain order, Cassandra checks different places that can have the data.</p>

<ul>
<li>The first one is the memtable.</li>
<li>If the data is not there, it checks the row key cache (if enabled),</li>
<li>then the bloom filter and</li>
<li>then the partition key cache (also if enabled).</li>
<li>If the partition key cache has the needed partition key, Cassandra goes straight to the compression offsets,</li>
<li>and after that it finally fetches the needed data out of a certain SSTable.</li>
<li>If the partition key wasn’t found in partition key cache, Cassandra checks the partition summary</li>
<li>and then the primary index before going to the compression offsets and extracting the data from the SSTable.</li>
</ul>

<p>After the data with the latest timestamp is located, it is fetched to the coordinator. Here, another stage of the read occurs. As we’ve stated here, Cassandra has issues with data consistency. The thing is that you write many data replicas and you may read their old versions instead of the newer ones. But Cassandra doesn’t ignore these consistency-related problems: it tries to solve them with a read repair process. The nodes that are involved in the read return results. Then, Cassandra compares these results based on the “last write wins” policy. Hence, the new data version is the main candidate to be returned to the user, while the older versions are rewritten to their nodes. But that’s not all. In the background, Cassandra checks the rest of the nodes that have the requested data (because the replication factor is often bigger than consistency level). When these nodes return results, the DB also compares them and the older ones get rewritten. Only after this, the user actually gets the result.</p>

<h3>Fully replicated</h3>

<p>You run Cassandra in a cluster with more than one nodes. Which not only gives reliability in case any node goes down but this reliability comes with the fact that data of each node is replicated on other nodes. That is why if any node goes down, other nodes start serving that data.</p>

<p><img src="/caas/caas-cluster.png" alt="alt text" /></p>

<ul>
<li>Client writes local</li>
<li>Data syncs across WAN</li>
<li>Replication factor per data center</li>
</ul>

<h2 id="h2-references-h2"><h2>References</h2></h2>

<ul>
<li><a href="https://docs.datastax.com/en/archived/cassandra/2.0/cassandra/dml/dml_write_path_c.html">https://docs.datastax.com/en/archived/cassandra/2.0/cassandra/dml/dml_write_path_c.html</a></li>
<li><a href="https://www.scnsoft.com/blog/cassandra-performance">https://www.scnsoft.com/blog/cassandra-performance</a></li>
</ul>

<p><strong>Note</strong>
For any question/suggestion, please send email to   <strong>amu.prashant1@gmail.com</strong></p>
]]></content>
        </item>
        
        <item>
            <title>Build your website using GoHugo and host it on GitHub Pages</title>
            <link>/posts/2019/08/build-your-website-using-gohugo-and-host-it-on-github-pages/</link>
            <pubDate>Mon, 12 Aug 2019 23:22:50 +0100</pubDate>
            
            <guid>/posts/2019/08/build-your-website-using-gohugo-and-host-it-on-github-pages/</guid>
            <description>Overview GitHub came up with GitHub pages service, which hosts static web content free of cost. For Tech bloggers and for those who are trying to make their hands dirty in static web content creation, this is the best service at this moment. Read tagline of GitHub Pages -
GoHugo is one of the most popular open-source static site generators. With its amazing speed and flexibility, GoHugo makes building websites fun again.</description>
            <content type="html"><![CDATA[

<h3 id="overview">Overview</h3>

<p><strong>GitHub</strong> came up with <a href="https://pages.github.com/">GitHub pages service</a>, which hosts static web content free of cost. For Tech bloggers and for those who are trying to make their hands dirty in static web content creation, this is the best service at this moment. Read tagline of GitHub Pages -</p>

<p><img src="/GitHub-pages-intro.PNG" alt="alt text" /></p>

<p><strong>GoHugo</strong> is one of the most popular open-source <a href="https://gohugo.io/">static site generators</a>. With its amazing speed and flexibility, GoHugo makes building websites fun again. It is <a href="https://themes.gohugo.io/">theme</a> based framework. You write markdown files and hugo creates static html by blending the content written in markdown files with the theme you selected. I am using <a href="https://themes.gohugo.io/hugo-theme-hello-friend-ng/">hello-friend-ng</a> theme for this website. And this entire post is written in single markdown file. Result is in-front of you. In future, I can choose any other theme and same content then will be presented differently on UI.</p>

<p><img src="/Hugo-intro.PNG" alt="alt text" /></p>

<p>Now, In this post I will explain the step by step procedure to create a personal blog site using Hugo and then hosting this on GitHubPages.</p>

<h3 id="step-1-repository-setup-in-github">Step 1 - Repository setup in GitHub</h3>

<h4 id="1-1-to-create-a-user-page-site-on-github-pages-create-a-repository-using-the-naming-scheme-lt-username-gt-github-io">1.1 - To create a User Page site on GitHub Pages, create a repository using the naming scheme  &lt;username&gt;.github.io.</h4>

<p>My GitHub username is prashantbhardwaj hence I created a repository with the name as <em>prashantbhardwaj.github.io</em>.
Now, when I will push any static web contents like HTML, CSS or JS files, those files will be hosted on GitHub Pages and can be accessed on this <a href="https://prashantbhardwaj.github.io">URL</a>. Please Note - Content from the master branch will be used to publish your GitHub Pages site.</p>

<h4 id="1-2-now-create-one-more-repository-to-keep-the-code-of-your-hugo-website-project">1.2 - Now create one more repository to keep the code of your Hugo Website project.</h4>

<p>GoHugo will be used to create static content of your website. To do so, you need a github repository to keep you Hugo project code. Finally that generated static content will be pushed into &lt;username&gt;.github.io repository. You can give any name for your GoHugo project repository. I chose <em>website</em>.</p>

<h4 id="please-note">Please note</h4>

<p>These two repository will remain blank until step 3. Later we will use these repositories to push static website content and the GoHugo code which will be used to generate these static web content.</p>

<h3 id="step-2-gohugo-installation">Step 2 - GoHugo installation</h3>

<p>To install GoHugo in your local machine, follow steps mentioned on this <a href="https://gohugo.io/getting-started/installing/">page</a>.</p>

<h3 id="step-3-learn-gohugo">Step 3 - Learn GoHugo</h3>

<p>Check this <a href="https://www.youtube.com/watch?v=qtIqKaDlqXo&amp;list=PLLAZ4kZ9dFpOnyRlyS-liKL5ReHDcj4G3">YouTube playlist</a>, to learn GoHugo, step by step. During this learning exercise you will also create your GoHugo website project.</p>

<h3 id="step-4-link-your-gohugo-project-to-your-github-repository">Step 4 - Link your GoHugo project to your github repository.</h3>

<p>I hope during learning GoHugo by watching the youtube clips mentioned in previous step, you created your project. Now, it is time to link your GoHugo project with your website project repository you created under step 1.2. Steps to link your project code with GitHub repository are mentioned on your GitHub repository home page. Open that home page on your GitHub repository website and follow them.</p>

<h3 id="step-5-link-your-theme-subproject-with-its-github-repository">Step 5 - Link your theme subproject with its github repository.</h3>

<p>I hope during learning GoHugo by watching the youtube clips mentioned in previous step, you created your project and if so then probably you downloaded your chosen theme from it&rsquo;s GitHub repository. For my project, I cloned hello-friend-ng theme from <a href="https://github.com/rhazdon/hugo-theme-hello-friend-ng.git">it&rsquo;s github repository</a> in my project&rsquo;s themes directory. As mentioned on <a href="https://themes.gohugo.io/hugo-theme-hello-friend-ng/">hello-friend-ng</a> website, I used following command to link this theme subproject with my main website project -</p>

<p><code>$ git submodule add https://github.com/rhazdon/hugo-theme-hello-friend-ng.git themes/hello-friend-ng</code></p>

<p>There is a benefit of linking your theme subproject with your theme&rsquo;s GitHub repository - whenever there will be any changes in your theme&rsquo;s code, you can very easily download them by using <code>git clone</code> command.</p>

<h3 id="step-6-generate-static-content-of-your-website">Step 6 - Generate static content of your website</h3>

<p><code>hugo -t theme-name</code> command generates static content of your website in <code>/public</code> directory. If you will link this directory with the repository created under step 1.1 the by just pushing that generated static web content, your website will be hosted on GitHubPages. Steps to link your <code>/public</code> with GitHub repository are mentioned on your GitHub repository home page. Open that home page on your GitHub repository website and follow them.</p>

<p>Now add this github project as submodule with your main website project. Detailed steps are mentioned <a href="https://gohugo.io/hosting-and-deployment/hosting-on-github/">here</a>.</p>

<h3 id="useful-links">Useful Links</h3>

<ul>
<li><a href="https://www.youtube.com/watch?v=qtIqKaDlqXo&amp;list=PLLAZ4kZ9dFpOnyRlyS-liKL5ReHDcj4G3">Videos to learn GoHugo</a></li>
<li><a href="https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#emphasis">Markdown cheatsheet</a></li>
<li><a href="https://gohugo.io/getting-started/installing/">GoHugo Installation steps</a></li>
<li><a href="https://gohugo.io/hosting-and-deployment/hosting-on-github/">GoHost Hugo site on GitHub</a></li>
</ul>
]]></content>
        </item>
        
    </channel>
</rss>
